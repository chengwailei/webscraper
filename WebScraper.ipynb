{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chengwailei/webscraper/blob/master/WebScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS1l4XT7dc-q",
        "colab_type": "text"
      },
      "source": [
        "**This code allow scraping of the indication, COVID-19 trial, and drug developer information from an excel file containing the DB code of candidate compounds**\n",
        "\n",
        "\n",
        "Source of indication: \n",
        "\n",
        "DrugBank (https://www.drugbank.ca/)\n",
        "ChEMBL API (https://github.com/chembl/chembl_webresource_client)\n",
        "\n",
        "\n",
        "Source of COVID-19 CT: \n",
        "\n",
        "ClinicalTrials.gov (https://clinicaltrials.gov/ct2/results?cond=COVID-19)\n",
        "\n",
        "\n",
        "Source of COVID-19 paper: \n",
        "\n",
        "COVID-19 Crowd Generated Gene and Drug Set Library (https://amp.pharm.mssm.edu/covid19/)\n",
        "\n",
        "\n",
        "Source of developer: \n",
        "\n",
        "Drug (https://www.drugs.com/pharmaceutical-companies.html\n",
        "\n",
        "Source of route:\n",
        "\n",
        "For ChEMBLID: INXIGHT:DRUGS (https://drugs.ncats.io/substances)\n",
        "\n",
        "For DB ID: DrugBank (https://www.drugbank.ca/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qAdmR_U0mjp",
        "colab_type": "text"
      },
      "source": [
        "House Keeping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6mMlfHed7qo",
        "colab_type": "code",
        "outputId": "8952d2de-9e26-475d-c5f6-6818ef2b4f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Install required packages\n",
        "!pip install requests\n",
        "!pip install openpyxl\n",
        "!pip install chembl_webresource_client\n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install xlsxwriter\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.6/dist-packages (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.6/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.6/dist-packages (from openpyxl) (1.0.1)\n",
            "Collecting chembl_webresource_client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/c4/6526156c7e2f164a0fc061aae20d383f0b6b1e79957510a64382e676e2dc/chembl-webresource-client-0.10.1.tar.gz (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from chembl_webresource_client) (1.24.3)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from chembl_webresource_client) (2.23.0)\n",
            "Collecting requests-cache>=0.4.7\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/55/9b1c40eb83c16d8fc79c5f6c2ffade04208b080670fbfc35e0a5effb5a92/requests_cache-0.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.6/dist-packages (from chembl_webresource_client) (1.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->chembl_webresource_client) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->chembl_webresource_client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->chembl_webresource_client) (2.9)\n",
            "Building wheels for collected packages: chembl-webresource-client\n",
            "  Building wheel for chembl-webresource-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chembl-webresource-client: filename=chembl_webresource_client-0.10.1-cp36-none-any.whl size=57153 sha256=aa39feb04c3deb91ae4a94cbe28d2715c04f8042df2fb9d0906cf29cf99745dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/8e/3b/4ec9940a01673307821600bfac28b17971caf84ff2b64653cb\n",
            "Successfully built chembl-webresource-client\n",
            "Installing collected packages: requests-cache, chembl-webresource-client\n",
            "Successfully installed chembl-webresource-client-0.10.1 requests-cache-0.5.2\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Hit:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,816 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [911 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,206 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [846 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [8,507 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,376 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [19.8 kB]\n",
            "Get:21 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [876 kB]\n",
            "Fetched 7,330 kB in 3s (2,532 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 47 not upgraded.\n",
            "Need to get 77.3 MB of archives.\n",
            "After this operation, 264 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 81.0.4044.138-0ubuntu0.18.04.1 [1,095 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 81.0.4044.138-0ubuntu0.18.04.1 [68.9 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 81.0.4044.138-0ubuntu0.18.04.1 [3,231 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 81.0.4044.138-0ubuntu0.18.04.1 [4,079 kB]\n",
            "Fetched 77.3 MB in 3s (24.2 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 144429 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_81.0.4044.138-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting xlsxwriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/1f/2092a81056d36c1b6651a645aa84c1f76bcee03103072d4fe1cb58501d69/XlsxWriter-1.2.8-py2.py3-none-any.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-1.2.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thOAGACvdGpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "import os\n",
        "import re\n",
        "from bs4 import NavigableString, Tag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhfw_Zo0OE_",
        "colab_type": "text"
      },
      "source": [
        "Make a Pharmaceutical Company List based on Drugs.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpqLeKyD6vMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Extract Pharmaceutical List from Drug.com\n",
        "import requests\n",
        "pharm_url = requests.get(\"https://www.drugs.com/pharmaceutical-companies.html\").text\n",
        "pharmsoup = BeautifulSoup(pharm_url)\n",
        "pharmList = []\n",
        "for pharm in pharmsoup.find_all('a',href=re.compile(r\"/manufacturer/\")):\n",
        "    pharm1 = pharm.text\n",
        "    pharm2 = pharm1.replace(\",\",\"\")\n",
        "    pharms = pharm2.replace(\".\",\"\")\n",
        "    pharmList.append(pharms.lower())\n",
        "# print(pharmList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loS80_8vQR33",
        "colab_type": "text"
      },
      "source": [
        "Extract COVID 19 Clinical Trial Table from Clinicaltrials.gov\n",
        "\n",
        "= Change max page if necessary\n",
        "\n",
        "While loop doesn't work well with Selenium\n",
        "\n",
        "= Alternative codes (expanded from while loop available below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2odUUXWmEdQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file2 = '/content/output_IP.csv'\n",
        "# clinical_trials = pd.read_csv(file2)\n",
        "# ## COVID19 Clinical Trial List from clinicaltrials.gov\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "# from selenium.webdriver.support.select import Select\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# import time\n",
        "\n",
        "# chrome_options = webdriver.ChromeOptions()\n",
        "# chrome_options.add_argument('--headless')\n",
        "# chrome_options.add_argument('--no-sandbox')\n",
        "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "# capa = DesiredCapabilities.CHROME\n",
        "# driver = webdriver.Chrome('chromedriver',options=chrome_options, desired_capabilities=capa)\n",
        "# driver.set_window_size(1440,900)\n",
        "# driver.get(\"https://clinicaltrials.gov/ct2/results?cond=COVID-19\")\n",
        "# page=1\n",
        "# max_page=12\n",
        "# Num=[]\n",
        "# Status=[]\n",
        "# StudyTitle=[]\n",
        "# StudyLink=[]\n",
        "# Conditions=[]\n",
        "# Interventions=[]\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[2]/label/select/option[4]\").click()\n",
        "\n",
        "\n",
        "# while page<=max_page:\n",
        "\n",
        "#   rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "#   for row in rows:\n",
        "#     Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#     Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#     StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#     StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#     Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#     Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "#     driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "#     time.sleep(60)\n",
        "#     page=page+1\n",
        "#     print('navigate to page: ' + str(page))\n",
        "\n",
        "# driver.close()\n",
        "\n",
        "# df=pd.DataFrame({\"Number\":Num,\"Interventions\":Interventions,\"Status\":Status,\"Study Title\":StudyTitle,\"Study Link\":StudyLink,\"Conditions\":Conditions})\n",
        "\n",
        "# print(df)\n",
        "# #df.to_csv('output_IP.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj51M8Mpu3km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## COVID19 Clinical Trial List from clinicaltrials.gov Exapnded code\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "# from selenium.webdriver.support.select import Select\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# import time\n",
        "\n",
        "# chrome_options = webdriver.ChromeOptions()\n",
        "# chrome_options.add_argument('--headless')\n",
        "# chrome_options.add_argument('--no-sandbox')\n",
        "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "# capa = DesiredCapabilities.CHROME\n",
        "# driver = webdriver.Chrome('chromedriver',options=chrome_options, desired_capabilities=capa)\n",
        "# driver.set_window_size(1440,900)\n",
        "# driver.get(\"https://clinicaltrials.gov/ct2/results?cond=COVID-19\")\n",
        "# page=1\n",
        "# # max_page=1\n",
        "# max_page=15\n",
        "# Num=[]\n",
        "# Status=[]\n",
        "# StudyTitle=[]\n",
        "# StudyLink=[]\n",
        "# Conditions=[]\n",
        "# Interventions=[]\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[2]/label/select/option[4]\").click()\n",
        "\n",
        "\n",
        "# # while page<=max_page:\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# rows= WebDriverWait(driver, 10000).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@id='theDataTable']/tbody//tr\")))\n",
        "# for row in rows:\n",
        "#   Num.append(row.find_element_by_xpath('./td[1]').text)\n",
        "#   Status.append(row.find_element_by_xpath('./td[3]').text)\n",
        "#   StudyTitle.append(row.find_element_by_xpath('./td[4]').text)\n",
        "#   StudyLink.append(row.find_element_by_xpath('./td[4]/a').get_attribute('href'))\n",
        "#   Conditions.append(row.find_element_by_xpath('./td[5]').get_attribute('textContent'))\n",
        "#   Interventions.append(row.find_element_by_xpath('./td[6]').get_attribute('textContent'))\n",
        "\n",
        "# driver.find_element_by_xpath(\"/html/body/div[6]/div[3]/div[2]/div[1]/div/div[2]/div[2]/div[6]/a[3]\").click()\n",
        "# time.sleep(60)\n",
        "# page=page+1\n",
        "# print('navigate to page: ' + str(page))\n",
        "\n",
        "# driver.close()\n",
        "\n",
        "# df=pd.DataFrame({\"Number\":Num,\"Interventions\":Interventions,\"Status\":Status,\"Study Title\":StudyTitle,\"Study Link\":StudyLink,\"Conditions\":Conditions})\n",
        "\n",
        "# print(df)\n",
        "# df.to_csv('output_IP.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbRihOyVVIFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## COVID Trial Table Cleaning\n",
        "file2 = './output_IP.csv'\n",
        "clinical_trials = pd.read_csv(file2)\n",
        "df1=clinical_trials\n",
        "my_str = df1['Interventions'].str\n",
        "substr = \"Drug:\"\n",
        "substr1 = \"Other\"\n",
        "substr2 = \"Device\"\n",
        "substr3 = \"Biological\"\n",
        "substr4 = \"Diagnostic\"\n",
        "substr5 = \"Procedure\"\n",
        "substr6 = \"Placebo:\"\n",
        "inserttxt = \"+\"\n",
        "df1['Interventions']=df1['Interventions'].str.replace(substr, inserttxt + substr)\n",
        "df1['Interventions']=df1['Interventions'].str.replace(substr1, inserttxt + substr1)\n",
        "df1['Interventions']=df1['Interventions'].str.replace(substr2, inserttxt +substr2)\n",
        "df1['Interventions']=df1['Interventions'].str.replace(substr3, inserttxt +substr3)\n",
        "df1['Interventions']=df1['Interventions'].str.replace(substr4, inserttxt +substr4)\n",
        "df1['Interventions']=df1['Interventions'].str.replace(substr5, inserttxt +substr5)\n",
        "df1['Interventions']=df1['Interventions'].str.replace(substr6, inserttxt +substr6)\n",
        "df1['Interventions']=df1['Interventions'].str.replace('+', '', 1)\n",
        "df1=df1.assign(var1=df1['Interventions'].str.split('+')).explode('var1')\n",
        "df1['Status']=df1['Status'].str.replace(\"\\nNEW\",\"\")\n",
        "df1.Status.unique()\n",
        "df1=df1[~df1.Status.str.contains(\"No longer available\")]\n",
        "df1=df1[~df1.Status.str.contains(\"Suspended\")]\n",
        "df1=df1[~df1.Status.str.contains(\"Terminated\")]\n",
        "df1 = df1[df1.Interventions.notnull()]\n",
        "df1 = df1[['var1', 'Interventions', 'Study Title', 'Study Link', 'Status','Conditions']]\n",
        "df1.columns = ['Drug', 'Interventions', 'Study Title', 'Study Link', 'Status','Conditions']\n",
        "df1['COVID19_Trials'] = df1[['Interventions','Study Title','Study Link']].astype(str).apply(lambda x: '\\n '.join(x), axis = 1)\n",
        "df1['Drug']=df1['Drug'].str.replace('Drug: ','')\n",
        "Clinical_trials = df1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqg49DNIi5e1",
        "colab_type": "text"
      },
      "source": [
        "Search Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLeoiYKmcJVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## COVID19 Clinical Trials Search\n",
        "def trialsearch(keyword):\n",
        "  keyword1=keyword.replace(r\"\\(.*\\)\",\"\")\n",
        "  result=Clinical_trials.loc[Clinical_trials['Drug'].str.contains(keyword1), 'COVID19_Trials'].astype(str)\n",
        "  # .str.contains('|'.join(substr), regex=True)]\n",
        "  result= ' |\\n'.join(result)\n",
        "  if (result==\"\"):\n",
        "    return \"No current trial\"\n",
        "  else:\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1FYLduKhOsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #In-vitro study search\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "# from selenium.webdriver.support.select import Select\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# from selenium.webdriver.common.keys import Keys\n",
        "# from selenium.webdriver import ActionChains\n",
        "# from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "# def invitrostudysearch(keyword):\n",
        "#     chrome_options = webdriver.ChromeOptions()\n",
        "#     chrome_options.add_argument('--headless')\n",
        "#     chrome_options.add_argument('--no-sandbox')\n",
        "#     chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "#     capa = DesiredCapabilities.CHROME\n",
        "#     driver = webdriver.Chrome('chromedriver',options=chrome_options, desired_capabilities=capa)\n",
        "#     driver.set_window_size(1440,900)\n",
        "#     driver.get(\"https://amp.pharm.mssm.edu/covid19/\")\n",
        "\n",
        "#     StudyName = []\n",
        "#     StudyLink = []\n",
        "\n",
        "#     # driver.find_element_by_id(\"pills-experimental-tab\").click();\n",
        "#     driver.find_element(By.XPATH,\"//*[@id='experimental_table_filter']/label/input\").click()\n",
        "#     driver.implicitly_wait(2)\n",
        "#     input_element = driver.find_element_by_css_selector('#experimental_table_filter > label > input[type=search]')\n",
        "#     action = ActionChains(driver)\n",
        "#     driver.implicitly_wait(2)\n",
        "#     action.move_to_element(input_element)\n",
        "#     action.click(input_element)\n",
        "#     action.send_keys(keyword).perform()\n",
        "#     page=1\n",
        "#     max_page=4\n",
        "\n",
        "#     while page<=max_page:\n",
        "#       rows= WebDriverWait(driver, 40).until(EC.visibility_of_all_elements_located((By.XPATH, \"/html/body/div[1]/div[4]/div/div/div[1]/div/div[1]/div/div/table/tbody/tr\")))\n",
        "\n",
        "#       for row in rows:\n",
        "#           try:\n",
        "#             elem=row.find_element_by_xpath('./td[2]')\n",
        "#             driver.implicitly_wait(2)\n",
        "#           except NoSuchElementException:\n",
        "#             pass\n",
        "#           try:\n",
        "#             StudyName.append(row.find_element_by_xpath('./td[2]').text)\n",
        "#             driver.implicitly_wait(2)\n",
        "#             content=row.find_element_by_xpath('./td[2]/div').get_attribute('data-content')  \n",
        "#             #  content1=re.search('<a href=\"(.*)\" target', content).group(1)      \n",
        "#             if \"<a href=\" in content:\n",
        "#               content1=re.search('<a href=\"(.*)\" target', content).group(1)\n",
        "#             else:\n",
        "#               content1=\"No Source Available\"\n",
        "#             StudyLink.append(content1)\n",
        "#           except NoSuchElementException:\n",
        "#             pass\n",
        "#       driver.find_element_by_id(\"experimental_table_next\").click();\n",
        "#       page=page+1\n",
        "\n",
        "#     driver.close()\n",
        "#     if len(StudyName)!= 0:\n",
        "#       InvitroStudy=pd.DataFrame({\"Study_Title\":StudyName,\"Study_Link\":StudyLink})\n",
        "#       InvitroStudy['Study_Link'] = '(' + InvitroStudy['Study_Link'].astype(str)\n",
        "#       InvitroStudy['Study_Link'] = InvitroStudy['Study_Link'].astype(str)+')'\n",
        "#       InvitroStudy = InvitroStudy.drop_duplicates(subset=['Study_Title']) \n",
        "#       InvitroStudy['Study'] = InvitroStudy[['Study_Title', 'Study_Link']].apply(lambda x: ''.join(x), axis=1)\n",
        "#       InvitroStudy = InvitroStudy.drop(InvitroStudy.columns[[0, 1]], axis=1) \n",
        "#       Invitro=InvitroStudy['Study'].str.cat(sep=' |\\n')\n",
        "#     else:\n",
        "#         Invitro=\"\"\n",
        "#     return Invitro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9H7fpGZ4R2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Insilico study search \n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "# from selenium.webdriver.support.select import Select    \n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# from selenium.webdriver.common.keys import Keys\n",
        "# from selenium.webdriver import ActionChains\n",
        "# from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "# def insilicostudysearch(keyword):\n",
        "#     chrome_options = webdriver.ChromeOptions()\n",
        "#     chrome_options.add_argument('--headless')\n",
        "#     chrome_options.add_argument('--no-sandbox')\n",
        "#     chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "#     capa = DesiredCapabilities.CHROME\n",
        "#     driver = webdriver.Chrome('chromedriver',options=chrome_options, desired_capabilities=capa)\n",
        "#     driver.set_window_size(1440,900)\n",
        "#     driver.get(\"https://amp.pharm.mssm.edu/covid19/\")\n",
        "\n",
        "#     StudyName = []\n",
        "#     StudyLink = []\n",
        "\n",
        "#     driver.find_element_by_id(\"pills-computational-tab\").click();\n",
        "#     driver.implicitly_wait(2)    \n",
        "#     driver.find_element(By.XPATH,\"//*[@id='experimental_table_filter']/label/input\").click()\n",
        "#     driver.implicitly_wait(2)\n",
        "#     input_element = driver.find_element_by_css_selector('#computational_table_filter > label > input[type=search]')\n",
        "#     driver.implicitly_wait(2)    \n",
        "#     action = ActionChains(driver)\n",
        "#     driver.implicitly_wait(2)\n",
        "#     action.move_to_element(input_element)\n",
        "#     driver.implicitly_wait(2)\n",
        "#     action.click(input_element)\n",
        "#     driver.implicitly_wait(2)    \n",
        "#     action.send_keys(keyword).perform()\n",
        "#     driver.implicitly_wait(2)    \n",
        "#     page=1\n",
        "#     max_page=4\n",
        "\n",
        "#     while page<=max_page:\n",
        "#       rows= WebDriverWait(driver, 40).until(EC.visibility_of_all_elements_located((By.XPATH, \"/html/body/div[1]/div[4]/div/div/div[1]/div/div[2]/div/div/table/tbody//tr\")))\n",
        "\n",
        "#       for row in rows:\n",
        "#           try:\n",
        "#             elem=row.find_element_by_xpath('./td[2]')\n",
        "#             driver.implicitly_wait(2)\n",
        "#           except NoSuchElementException:\n",
        "#             pass\n",
        "#           try:\n",
        "#              StudyName.append(row.find_element_by_xpath('./td[2]').text)\n",
        "#              driver.implicitly_wait(2)\n",
        "#              content=row.find_element_by_xpath('./td[2]/div').get_attribute('data-content')\n",
        "#              driver.implicitly_wait(2)            \n",
        "#              content1=re.search('<a href=\"(.*)\" target', content).group(1)\n",
        "#              driver.implicitly_wait(2)    \n",
        "#              StudyLink.append(content1)\n",
        "#           except NoSuchElementException:\n",
        "#             pass\n",
        "      \n",
        "#       driver.find_element_by_id(\"computational_table_next\").click();\n",
        "#       page=page+1\n",
        "\n",
        "#     driver.close()\n",
        "#     if len(StudyName)!= 0:\n",
        "#       InsilicoStudy=pd.DataFrame({\"Study_Title\":StudyName,\"Study_Link\":StudyLink})\n",
        "#       InsilicoStudy['Study_Link'] = '(' + InsilicoStudy['Study_Link'].astype(str)\n",
        "#       InsilicoStudy['Study_Link'] = InsilicoStudy['Study_Link'].astype(str)+')'\n",
        "#       InsilicoStudy = InsilicoStudy.drop_duplicates(subset=['Study_Title']) \n",
        "#       InsilicoStudy['Study'] = InsilicoStudy[['Study_Title', 'Study_Link']].apply(lambda x: ''.join(x), axis=1)\n",
        "#       InsilicoStudy = InsilicoStudy.drop(InsilicoStudy.columns[[0, 1]], axis=1) \n",
        "#       Insilico=InsilicoStudy['Study'].str.cat(sep=' |\\n')\n",
        "#     else:\n",
        "#         Insilico=\"\"\n",
        "#     return Insilico"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDwge1-HznmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ChEMBL Indication Search:\n",
        "from chembl_webresource_client.new_client import new_client\n",
        "\n",
        "def chemblindication(keyword):\n",
        "  drug_indication = new_client.drug_indication\n",
        "  mol_id = drug_indication.filter(molecule_chembl_id__icontains=format(keyword)).only(['mesh_heading', 'max_phase_for_ind'])\n",
        "  chembldrugindi  = '-'.join([str(counterofobjects) for counterofobjects in mol_id])\n",
        "  chembldrugindi.rsplit(\"}-{\")\n",
        "  chembldrugindiList1=chembldrugindi.replace(\"'mesh_heading':\",\"\")\n",
        "  chembldrugindiList2=chembldrugindiList1.replace(\"'max_phase_for_ind': \",\"Phase\")\n",
        "  chembldrugindiList3=chembldrugindiList2.replace(\"'}-\",\"|\")\n",
        "  indication1 = chembldrugindiList3.replace(\"{\",\" \")\n",
        "  indication2 = indication1.replace(\"}\",\" \")\n",
        "  indication = indication2.replace(\"'\",\" \")\n",
        "  return indication"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOl9eHrkt-5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # from chembl_webresource_client.new_client import new_client\n",
        "\n",
        "# # def chemblindicationID(keyword):\n",
        "#   drug_indication = new_client.drug_indication\n",
        "#   mol_id = drug_indication.filter(molecule_chembl_id__icontains=format(keyword)).only(['mesh_id'])\n",
        "#   chembldrugindi  = '-'.join([str(counterofobjects) for counterofobjects in mol_id])\n",
        "#   chembldrugindi.rsplit(\"}-{\")\n",
        "#   chembldrugindiList1=chembldrugindi.replace(\"'mesh_id':\",\"\")\n",
        "#   # chembldrugindiList2=chembldrugindiList1.replace(\"'max_phase_for_ind': \",\"Phase\")\n",
        "#   # chembldrugindiList3=chembldrugindiList2.replace(\"'}-\",\"|\")\n",
        "#   # indication1 = chembldrugindiList3.replace(\"{\",\" \")\n",
        "#   # indication2 = indication1.replace(\"}\",\" \")\n",
        "#   # indication = indication2.replace(\"'\",\" \")\n",
        "#   # return indication"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Poty__MVOH7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ChEMBL ID\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "from selenium.webdriver.support.select import Select\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver import ActionChains\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "def searchchemblid(keyword):\n",
        "    \n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    capa = DesiredCapabilities.CHROME\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options, desired_capabilities=capa)\n",
        "    driver.set_window_size(1440,900)\n",
        "    url = (\"https://www.drugbank.ca/{}\").format(keyword)\n",
        "    driver.get(url)\n",
        "\n",
        "    try:\n",
        "        chemblID1=driver.find_element_by_partial_link_text('CHEMBL')\n",
        "        chemblID1=chemblID1.get_attribute(\"href\")\n",
        "        chemblID=chemblID1.rpartition('/')[-1]\n",
        "    except NoSuchElementException:\n",
        "      chemblID=keyword\n",
        "      pass\n",
        "    driver.close()\n",
        "    return(chemblID)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8G83DQ0gjfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comparer(L1, L2):\n",
        "    for i in L2:\n",
        "        for j in L1:\n",
        "            if (i in j) or (j in i):\n",
        "                yield j"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVEz2lYQRnDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DB Developer Search\n",
        "def searchdeveloper(keyword):\n",
        "    from urllib.request import urlopen\n",
        "    url = \"https://www.drugbank.ca/drugs/{}\".format(keyword)\n",
        "    response = urlopen(url)\n",
        "    soup = BeautifulSoup(response)\n",
        "\n",
        "    manufacturer=[]\n",
        "\n",
        "    for manlist in soup.find_all(attrs={\"class\":\"manufacturer-columns\"}):\n",
        "      for man in manlist.find_all('li'):\n",
        "        man1=man.text\n",
        "        man2=man1.replace(\",\",\"\")\n",
        "        mans=man2.replace(\".\",\"\")\n",
        "        manufacturer.append(mans.lower())\n",
        "    packager = []\n",
        "    for packlist in soup.find_all(attrs={\"class\":\"packager-columns\"}):\n",
        "      for pack in packlist.find_all('li'):\n",
        "        pack1=pack.text\n",
        "        pack2=pack1.replace(\",\",\"\")\n",
        "        packs=pack2.replace(\".\",\"\")\n",
        "        packager.append(packs.lower())\n",
        "    manpack = manufacturer + packager\n",
        "    manpackList = []\n",
        "    for word in manpack:\n",
        "      if word not in manpackList:\n",
        "        manpackList.append(word)\n",
        "    match1 = list(comparer(pharmList, manpackList))\n",
        "    match = \"|\".join(str(x) for x in match1)\n",
        "    return(match)\n",
        "    # print(match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwmIwDHDS0Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "def searchDBIndication(keyword):\n",
        "  url = \"https://www.drugbank.ca/drugs/{}\".format(keyword)\n",
        "  response = urlopen(url)\n",
        "  soup = BeautifulSoup(response)\n",
        "  indilist = []\n",
        "  extr = soup.find('dt', string='Indication')\n",
        "  indication = extr.find_next_sibling('dd').text\n",
        "  return(indication)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf9DzyCXpqlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "def searchrouteDB(keyword):\n",
        "  url = \"https://www.drugbank.ca/drugs/{}\".format(keyword)\n",
        "  response = urlopen(url)\n",
        "  soup = BeautifulSoup(response)\n",
        "  indilist = []\n",
        "  extr = soup.find('dt', string='Prescription Product')\n",
        "  extr2= extr.find_next_sibling('table')\n",
        "\n",
        "  return(extr2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXALuHa7q3Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "def searchrouteDB(keyword):\n",
        "  url = \"https://www.drugbank.ca/drugs/{}\".format(keyword)\n",
        "  routelist = []\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  extr = soup.find('dt', string='Prescription Products')\n",
        "  if extr is not None:\n",
        "    extr2 = extr.find_next_sibling('dd')\n",
        "    extr3 = extr2.find_all('table')[0].tbody.find_all('tr')\n",
        "    for row in extr3:\n",
        "      information = row.find_all('td')[3].text\n",
        "      routelist.append(information)\n",
        "  else:\n",
        "    extr = soup.find('dt', string='Over the Counter Products')\n",
        "    if extr is not None:\n",
        "      extr2 = extr.find_next_sibling('dd')\n",
        "      extr3 = extr2.find_all('table')[0].tbody.find_all('tr')\n",
        "      for row in extr3:\n",
        "        information = row.find_all('td')[3].text\n",
        "        routelist.append(information)  \n",
        "    else:\n",
        "      routelist=''\n",
        "  routelist1 = []\n",
        "  for word in routelist:\n",
        "    if word not in routelist1:\n",
        "      routelist1.append(word)\n",
        "  route = \"|\".join(str(x) for x in routelist1)\n",
        "  return(route)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erRzQaQeAEk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "def searchrouteCHEMBL(keyword):\n",
        "  url = \"https://www.ebi.ac.uk/chembl/assay_report_card/{}\".format(keyword)\n",
        "  routelist = []\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  extr = soup.find('dt', string='Prescription Products')\n",
        "  if extr is not None:\n",
        "    extr2 = extr.find_next_sibling('dd')\n",
        "    extr3 = extr2.find_all('table')[0].tbody.find_all('tr')\n",
        "    for row in extr3:\n",
        "      information = row.find_all('td')[3].text\n",
        "      routelist.append(information)\n",
        "  else:\n",
        "    extr = soup.find('dt', string='Over the Counter Products')\n",
        "    if extr is not None:\n",
        "      extr2 = extr.find_next_sibling('dd')\n",
        "      extr3 = extr2.find_all('table')[0].tbody.find_all('tr')\n",
        "      for row in extr3:\n",
        "        information = row.find_all('td')[3].text\n",
        "        routelist.append(information)  \n",
        "    else:\n",
        "      routelist=''\n",
        "  routelist1 = []\n",
        "  for word in routelist:\n",
        "    if word not in routelist1:\n",
        "      routelist1.append(word)\n",
        "  route = \"|\".join(str(x) for x in routelist1)\n",
        "  return(route)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC1ir_7rAOeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NIH route search \n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "from selenium.webdriver.support.select import Select    \n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver import ActionChains\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "def searchrouteNIH(keyword):\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    capa = DesiredCapabilities.CHROME\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options, desired_capabilities=capa)\n",
        "    driver.set_window_size(1440,900)\n",
        "    driver.get(\"https://drugs.ncats.io/substances\")\n",
        "\n",
        "    driver.implicitly_wait(2)    \n",
        "    driver.find_element(By.XPATH,\"/html/body/div[2]/nav/div[1]/div[2]/ul/li[1]/form/div/input\").click()\n",
        "    driver.implicitly_wait(2)\n",
        "    input_element = driver.find_element_by_css_selector('#search')\n",
        "    driver.implicitly_wait(2)    \n",
        "    action = ActionChains(driver)\n",
        "    driver.implicitly_wait(2)\n",
        "    action.move_to_element(input_element)\n",
        "    driver.implicitly_wait(2)\n",
        "    action.click(input_element)\n",
        "    driver.implicitly_wait(2)    \n",
        "    action.send_keys(keyword).perform()\n",
        "    driver.implicitly_wait(2)\n",
        "    driver.find_element(By.XPATH,\"/html/body/div[2]/nav/div[1]/div[2]/ul/li[1]/form/div/span/div/button\").click()    \n",
        "    driver.implicitly_wait(60)\n",
        "    driver.find_element_by_css_selector(\"#card-title > h3\").click()\n",
        "    driver.implicitly_wait(30)\n",
        "    currentURL = driver.current_url\n",
        "    driver.close()\n",
        "    page = requests.get(currentURL)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    route1 = soup.find_all('div',{'class':'subsub-title'})[0].find_all('span')[0].text\n",
        "    if route1 is not None:\n",
        "      route2 = route1.replace(\"\\n\",\"\")\n",
        "      route = route2.replace(\" \",\"\")\n",
        "    else:\n",
        "      route = ''\n",
        "    return route"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH33lEvaIsEu",
        "colab_type": "text"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ee3GOjbU1g0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "e7d2f7b7-50b6-444b-c3bc-0b09b03fdb19"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from chembl_webresource_client.new_client import new_client\n",
        "DBstring = \"DB\"\n",
        "CHEMBLstring = \"CHEMBL\"\n",
        "\n",
        "## Read excel file containing drug names\n",
        "file = './COVID19_candidate_approved_drugs_combined_mti_pharm_targets.v0.xlsx'\n",
        "df_c = pd.read_excel(file, sheet_name=0)\n",
        "# df_6 = pd.read_excel(file, sheet_name=1)\n",
        "# df_24 = pd.read_excel(file, sheet_name=2)\n",
        "\n",
        "## Individual ID list\n",
        "commonlist= df_c.iloc[:, 0].tolist()\n",
        "# sixlist= df_6.iloc[:, 0].tolist()\n",
        "# tflist= df_24.iloc[:, 0].tolist()\n",
        "# # print(commonlist[0:40])\n",
        "\n",
        "dfa=df_c\n",
        "dfa.columns.values[0]=\"ID\"\n",
        "\n",
        "# SearchedIDList = []\n",
        "\n",
        "# for i in commonlist:\n",
        "#   if CHEMBLstring not in i:  \n",
        "#     ## ChEMBL ID\n",
        "#     searchedID=searchchemblid(i)\n",
        "#     if searchedID==\"NA\":\n",
        "#         searchedID==i\n",
        "\n",
        "#     ## Developer \n",
        "#     # developer=searchdeveloper(i)\n",
        "#     # DeveloperList.append(developer)\n",
        "\n",
        "#   else:\n",
        "#     ## ChEMBL ID\n",
        "#     searchedID = i\n",
        "#     SearchedIDList.append(searchedID)\n",
        "\n",
        "\n",
        "# print(SearchedIDList)\n",
        "# # print(commonlist)\n",
        "  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a715d21a0314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## Read excel file containing drug names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./COVID19_candidate_approved_drugs_combined_mti_pharm_targets.v0.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# df_6 = pd.read_excel(file, sheet_name=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# df_24 = pd.read_excel(file, sheet_name=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './COVID19_candidate_approved_drugs_combined_mti_pharm_targets.v0.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItAH2eevtM_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "warnings.filterwarnings(\"ignore\", 'This pattern has match groups')\n",
        "DBstring = \"DB\"\n",
        "CHEMBLstring = \"CHEMBL\" \n",
        "file = './COVID19_candidate_approved_drugs_combined.v1.xlsx'\n",
        "df_c = pd.read_excel(file, sheet_name=0)\n",
        "# df_6 = pd.read_excel(file, sheet_name=1)\n",
        "# df_24 = pd.read_excel(file, sheet_name=2)\n",
        "dfa=df_c\n",
        "# dfb=df_6\n",
        "# dfc=df_24\n",
        "\n",
        "dfa.columns.values[0]=\"ID\"\n",
        "dfa['SearchedIDList']=dfa['ID'].apply(lambda x: x if CHEMBLstring in x else searchchemblid(x))\n",
        "dfa['Indication']=dfa['SearchedIDList'].apply(lambda x : chemblindication(x) if CHEMBLstring in x else '')\n",
        "dfa['Indication Paragraph']=dfa['ID'].apply(lambda x : searchDBIndication(x) if DBstring in x else '')\n",
        "dfa['COVID-19 Clinical Trials']=dfa['name'].apply(trialsearch)\n",
        "dfa['Developer']=dfa['ID'].apply(lambda x: searchdeveloper(x) if DBstring in x else '')\n",
        "dfa['Route']=dfa['ID'].apply(lambda x: searchrouteDB(x) if DBstring in x else pd.isnull)\n",
        "dfa['Route'] = dfa.apply(lambda row: searchrouteNIH(row['name']) if pd.isnull(row['Route']) else row['Route'],axis=1)\n",
        "# # dfa['COVID-19 In Vitro']=dfa['name'].apply(invitrostudysearch)\n",
        "# # dfa['COVID-19 In Silico']=dfa['name'].apply(insilicostudysearch)\n",
        "\n",
        "# dfb.columns.values[0]=\"ID\"\n",
        "# dfb['SearchedIDList']=dfb['ID'].apply(lambda x: x if CHEMBLstring in x else searchchemblid(x))\n",
        "# dfb['Indication']=dfb['SearchedIDList'].apply(lambda x : chemblindication(x) if CHEMBLstring in x else '')\n",
        "# dfb['Indication Paragraph']=dfb['SearchedIDList'].apply(lambda x : searchDBIndication(x) if DBstring in x else '')\n",
        "# dfb['COVID-19 Clinical Trials']=dfb['name'].apply(trialsearch)\n",
        "# dfb['Developer']=dfa['ID'].apply(lambda x: searchdeveloper(x) if DBstring in x else '')\n",
        "# dfb['Route']=dfb['ID'].apply(lambda x: searchrouteDB(x) if DBstring in x else pd.isnull)\n",
        "# dfb['Route'] = dfb.apply(lambda row: searchrouteNIH(row['name']) if pd.isnull(row['Route']) else row['Route'],axis=1)\n",
        "# # # # dfb['COVID-19 In Vitro']=dfb['name'].apply(invitrostudysearch)\n",
        "# # # # dfb['COVID-19 In Silico']=dfb['name'].apply(insilicostudysearch)\n",
        "\n",
        "# dfc.columns.values[0]=\"ID\"\n",
        "# dfc['SearchedIDList']=dfc['ID'].apply(lambda x: x if CHEMBLstring in x else searchchemblid(x))\n",
        "# dfc['Indication']=dfc['SearchedIDList'].apply(lambda x : chemblindication(x) if CHEMBLstring in x else '')\n",
        "# dfc['Indication Paragraph']=dfc['SearchedIDList'].apply(lambda x : searchDBIndication(x) if DBstring in x else '')\n",
        "# dfc['COVID-19 Clinical Trials']=dfc['name'].apply(trialsearch)\n",
        "# dfc['Developer']=dfc['ID'].apply(lambda x: searchdeveloper(x) if DBstring in x else '')\n",
        "# dfc['Route']=dfc['ID'].apply(lambda x: searchrouteDB(x) if DBstring in x else pd.isnull)\n",
        "# dfc['Route'] = dfc.apply(lambda row: searchrouteNIH(row['name']) if pd.isnull(row['Route']) else row['Route'],axis=1)\n",
        "# # # # dfc['COVID-19 In Vitro']=dfc['name'].apply(invitrostudysearch)\n",
        "# # # # dfc['COVID-19 In Silico']=dfc['name'].apply(insilicostudysearch)\n",
        "\n",
        "writer = pd.ExcelWriter('COVID19_candidate_approved_drugs_combined_mti_pharm_targets_winnie.v1.xlsx', engine='xlsxwriter')\n",
        "dfa.to_excel(writer, sheet_name='')\n",
        "# dfb.to_excel(writer, sheet_name='Sheet2')\n",
        "# dfc.to_excel(writer, sheet_name='Sheet3')\n",
        "writer.save()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-MJbg0yKUIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}